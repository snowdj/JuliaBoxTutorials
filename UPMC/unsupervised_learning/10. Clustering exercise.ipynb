{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering exercises\n",
    "\n",
    "\n",
    "### Get data\n",
    "\n",
    "In this notebook we will try out clustering techniques on our fruit and mystery datasets. Let's start by loading in our data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuliaDB, MNIST \n",
    "\n",
    "rescale(A, dim::Integer=1) = (A .- mean(A, dim)) ./ max.(std(A, dim), eps())\n",
    "\n",
    "# Fruit dataset\n",
    "fruit_table = loadtable([\"../training/data/Apple_Golden_1.dat\",\"../training/data/bananas.dat\"]; delim = '\\t', filenamecol=:apple => (x) -> x == \"../training/data/Apple_Golden_1.dat\" ? true : false)\n",
    "\n",
    "matdata = hcat(columns(fruit_table)...)\n",
    "fruit_data = rescale(matdata[:,2:end], 1)\n",
    "fruit_labels = ifelse.(matdata[:,1].==1, \"Apple\",\"Banana\")\n",
    "\n",
    "# Mystery dataset\n",
    "mystery_data, labels = traindata()\n",
    "N = 2500\n",
    "mystery_data = rescale(convert(Matrix{Float64}, mystery_data[:, 1:N])',1)\n",
    "mystery_labels = Int.(labels)[1:N];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting purposes, let's keep our PCA results from the last notebook around (for plotting our data along the axes with the greatest variation to visualize clusterings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MultivariateStats\n",
    "M = fit(PCA, fruit_data', maxoutdim = 4)\n",
    "fruit_PCA = transform(M, fruit_data')'\n",
    "M1 = fit(PCA, mystery_data')\n",
    "mystery_PCA = transform(M1, mystery_data')';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "Let's use `kmeans` from the `Clustering` package to find the natural groupings in that dataset. Recall that `k`, the number of clusters has to be chosen by the user. Since we know we have apples and bananas, choose `k=2` and plot the different clusters in different colors.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should see that the clusters found the two groups: the apples and the bananas. It doesn't know which is which, but after labeling our data we can go back in and see what it found. \n",
    "\n",
    "However, with our mystery data we don't know how many clusters there are. So let's try some different values of `k` and see how it looks.\n",
    "\n",
    "Try `k = 2, 5, 10, 15` and plot the results. Are you able to see if this clustering seems to be working?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow method\n",
    "\n",
    "One way to choose the number of clusters is to use the *elbow method*. Like visual inspection of our cluster results, the elbow method is not very rigorous, but sometimes it can give us a bit more information.\n",
    "\n",
    "The idea is that our clustering has done well if we have (1) minimized the distance between cluster centers and data points associated with that cluster and (2) maximized the distance between cluster centers. \n",
    "\n",
    "The output object of `kmeans` has fields called `costs` and `totalcost`:\n",
    "\n",
    "```julia\n",
    "out = kmeans(somedata, k)\n",
    "costs = out.costs\n",
    "totalcost = out.totalcost\n",
    "```\n",
    "\n",
    "The `totalcost` is the sum of distances of all points in the dataset from their respective cluster centers. We can plot how this value changes as the number of cluster centers increases to choose `k`. The hope is that our ideal `k` will be the value at which there is an **elbow** in the `total_cost` vs. `k` curve, i.e. where decreases in `total_cost` fall off.\n",
    "\n",
    "Why can't we simply minimize the distance between data points and their cluster centers?\n",
    "\n",
    "Plot `total_cost` vs. `k` for values of `k` between 2 and 30.\n",
    "\n",
    "Are you able to tell how many clusters you want?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we don't seem to be able to learn much about our `mystery_data` from the elbow method. Maybe kmeans is not the best approach to deal with this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
