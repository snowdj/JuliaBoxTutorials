{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional reduction\n",
    "\n",
    "Big data has high **dimensionality**, or a large number of features. (Each feature represents one dimension.)\n",
    "\n",
    "For example, Netflix's data contains ratings for millions of movies. Even a small single picture with only `100` by `100` pixels has `3 x 100 x 100 = 30,000` numbers that describe it. (Each pixel has three color channels -- red, green, and blue!)\n",
    "\n",
    "How can we work with thousands or even millions of dimensions?\n",
    "\n",
    "One answer is to do dimensional reduction. Datasets usually contain somewhat redundant information; if we can remove some of this redundancy and distill our dataset to something simpler, it may be easier for us to work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components analysis\n",
    "\n",
    "#### Concept\n",
    "\n",
    "**Principal Components Analysis** (PCA) is the simplest form of dimensionality reduction. We can think of PCA as a process of rotating a coordinate system such that the axes will go through or align with the data better.\n",
    "\n",
    "Let's consider the following graphic:\n",
    "\n",
    "![PCA](https://newonlinecourses.science.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/lesson05/PCA_plot/index.gif)\n",
    "\n",
    "In this image, the data falls on a two dimensional plane, but it has obvious structure: we see that values along the vertical axis roughly increase as we move from left to right. Our data trends with a line going from the bottom left corner of the image to the top right. Our data also exhibits some spread along the direction going from the upper left corner of the image to the lower right. \n",
    "\n",
    "In PCA, these directions become the first and second **principal components** (PCs). These principal components give us a more natural way to express our data. \n",
    "\n",
    "Furthermore, PCA will order the principal components in terms of importance. This means you can reduce the number of axes or dimensions in your data set to however many are required to give the desired accuracy.\n",
    "\n",
    "Here the first principal component is the direction (originally from the lower left corner to the upper right) along which our data primarily falls. If we'd like, we can ditch the second (less important) principal component and work with our originally 2D dataset as a 1D dataset.\n",
    "\n",
    "In general, after expressing our dataset in terms of its principal components, we can choose to describe/reconstruct our dataset using only a subset of those principal components.\n",
    "\n",
    "#### Underlying math\n",
    "\n",
    "In linear algebra terms, this amounts to performing a change of basis via a singular value decomposition (SVD).\n",
    "\n",
    "For those of you that are familiar with SVDs, PCA's principal components are singular vectors, each of which has an associated \"singular value\". These singular values quantify the relative importance of each component. Components associated with large singular values are more important. We will not lose very much information by excluding from the reconstruction of our dataset principal components associated with relatively small singular values.\n",
    "\n",
    "#### Why does this work?\n",
    "\n",
    "In theory, the data can be perfectly random and there is no \"good axis\". However, **manifold hypothesis** conjectures that there always is a \"good axis\". The explanation goes like this: in high dimensional data, the different pieces/features of that data are related. \n",
    "\n",
    "For example, a person's income is related to the person's education. The person's rating of Disney movies is related to their rating of Pixar movies. The amount of blue in a pixel is related to the amount of green in images of fruits. These relations between data means that the data isn't randomly distributed, but rather it actually should have structure. That structure may be high-dimensional, in which case we can't picture it. This general idea of \"structure\" or \"shape\" is what is mathematically referred to as a **manifold**, i.e. a \"curved surface\" in the high-dimensional space. Here are a few illustrations of a manifold:\n",
    "\n",
    "![manifold1](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/BoysSurfaceTopView.PNG/220px-BoysSurfaceTopView.PNG)\n",
    "![manifold2](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/unlink-2spiral.png)\n",
    "\n",
    "You can imagine that, if you could plot your data in 100,000 dimensional space, you would be able to visualize how different features change together. However, we are stuck with 2D and 3D images. Dimensional reduction techniques work by finding the lines that go through these shapes, capturing most of the information with much lower dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
