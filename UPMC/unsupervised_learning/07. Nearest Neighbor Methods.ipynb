{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Methods\n",
    "\n",
    "Methods to determine nearest neighbors are common in machine learning, both for supervised and unsupervised learning. In this notebook, we'll talk about KD Trees (a way to find nearest neighbors) and then we'll go over kNN, a supervised technique that uses nearest neighbors to classify.\n",
    "\n",
    "### KD Trees\n",
    "\n",
    "Building a KD Tree is one method for finding a data point's nearest neighbors. In this method, a `k` dimensional space is partitioned by bisecting the space iteratively along each of its dimensions. (In each iteration, bisection breaks the data in space roughly in half at the dataset's median value for the `k`th dimension.) \n",
    "\n",
    "We will consider a two dimensional space with the following data points in light blue:\n",
    "\n",
    "<img src=\"images/points_before_tree.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Let us first partition the space along the horizontal axis (with a red vertical line) at the median point along the horizontal axis. After partitioning at the median along the horizontal axis, we'll partition at the median along the vertical axis and then repeat until we have no more than two points in any unpartitioned space. In doing this, we'll end up with space partitioned like so:\n",
    "\n",
    "<img src=\"images/exact_median_partition.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "In the above image, it's not clear where the bisected points fall, so let's redraw this image with shifted lines that cause points to fall to the top and right of any bisecting lines.\n",
    "\n",
    "<img src=\"images/red_first_partition.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Notice that some of our predicted nearest neighbors (points falling in the same square) are correct and some are not:\n",
    "\n",
    "<img src=\"images/NN_predictions_red_first.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Also note that we would have gotten a different result if we had started partitioning along a different dimension:\n",
    "\n",
    "<img src=\"images/blue_first_partition.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/NN_predictions_blue_first.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "## Nearest Neighbor Methods in Classification\n",
    "\n",
    "Take a look at this picture.\n",
    "\n",
    "![NN](https://annalyzin.files.wordpress.com/2016/09/knn-layman-explanation-borderless.png?w=350&h=200&crop=1)\n",
    "\n",
    "The `?` in the middle: should it be red or black? The answer is pretty intuitive: it is next to a buch of red so it is most likely red. That idea itself is a classification algorithm known as nearest neighbors. But to have a computer do this, we need to clearly write down what rules we followed.\n",
    "\n",
    "### K-Nearest Neighbors (kNN)\n",
    "\n",
    "In K-nearest neighbors (kNN), we choose $k$, the number of nearest neighbors to consider, and classify a data point according to the majority behavior of its $k$ closest neighbors. \n",
    "\n",
    "In the above example, we chose `k = 5`. We looked at the 5 points closest to our mystery point, saw that 4 were red and 1 was black, and therefore guessed our mystery point was red.\n",
    "\n",
    "This algorithm has a lot of upsides:\n",
    "\n",
    "1. It's simple, easy to understand, and matches intuition.\n",
    "2. It's easy to implement.\n",
    "3. It maps over to new data relatively quickly (no \"training\").\n",
    "\n",
    "Thus while it may not have as high of an accuracy as some methods like neural networks on complicated problems, it's a fairly simple way to get a good interpretable solution and thus can be an appropriate choice for many problems.\n",
    "\n",
    "### Variants of kNN: Weighted kNN\n",
    "\n",
    "What if the 5th point is very far away? Intuitively you'd think that you'd care less about the neighbors which are further away. This addition to the algorithm, weighing by distance, is called the weighted kNN algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
