{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering methods\n",
    "\n",
    "Clustering methods break our data up into underlying groups based on shared characteristics. There are many, many different clustering algorithms, but let's talk about **kmeans** and **DBSCAN** (Density Based Spatial Clustering of Applications) to see how they generally work.\n",
    "\n",
    "### Kmeans clustering\n",
    "\n",
    "#### Concept\n",
    "\n",
    "$K$-means clustering is the most basic and most common clustering algorithm. You must feed the algorithm the number of clusters, `k`, and then the algorithm spits back `k` different data clusters. \n",
    "\n",
    "#### How does this work?\n",
    "\n",
    "The algorithm starts by selecting `k` cluster locations randomly. Each point in the dataset is then assigned to its closest cluster. Once all points have been assigned to clusters, the center of each cluster is determined by averaging the locations of all points assigned to that cluster. All points in the dataset are reassigned to their closest cluster. This is done iteratively until the algorithm converges.\n",
    "\n",
    "A diagramatic description is the following:\n",
    "\n",
    "![kmeans](https://i.ytimg.com/vi/_aWzGGNrcic/hqdefault.jpg)\n",
    "\n",
    "Here is an example of data plotted in different colors based on cluster associations, allowing us to see natural groupings.\n",
    "\n",
    "![kmeans2](https://i.imgur.com/S65Sk9c.jpg)\n",
    "\n",
    "#### How many clusters?\n",
    "\n",
    "Since kmeans only gives you the best way to group your data into `k` groups, how do you determine how many groups there are? In practice you would run kmeans clustering for many values of `k` and then see which value of `k` works best! \n",
    "\n",
    "We want our choice of `k` to decrease the average distance, $d$, between each point and their cluster centers, but if we take this too far, we'll end up with each point as its own cluster. If, for each value of `k`, we plot `k`$\\times d$, we'll typically see that `k`$\\times d$ decreases markedly as `k` increases until the optimal number for `k` is tried. After that, `k`$\\times d$ will not decrease as much.\n",
    "\n",
    "### DBSCAN (Density Based Spatial Clustering of Applications)\n",
    "\n",
    "DBSCAN uses density-based clustering to determine groupings. This algorithm requires two inputs -- a distance $\\epsilon$ to define neighborhoods and a minimum number of neighbors, $n$, required in a neighborhood to consider that neighborhood a cluster.\n",
    "\n",
    "#### How does this work?\n",
    "\n",
    "In this algorithm, you start with a point in your dataset and you draw a small circle around that point (defined by distance $\\epsilon$ from the point). If at least $n$ neighbors are contained in the circle, you have a cluster! Now you'll expand the cluster by looking at the neighborhoods (within distance $\\epsilon$) of each point in the cluster, and adding new points contained in each neighborhood to the cluster until you run out of neighbors with enough data point. At this stage, you've reached the end of the cluster and can repeat the above with a new starting point.\n",
    "\n",
    "This algorithm may be visualized with the following:\n",
    "\n",
    "![dbscan](http://slideplayer.com/6400248/22/images/7/Clustering+Example+%28DBSCAN%5B1%5D%29.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
