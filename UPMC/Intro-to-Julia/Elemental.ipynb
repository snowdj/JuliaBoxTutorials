{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Elemental.jl\n",
    "\n",
    "`Elemental.jl` is a Julia package that provides Julia bindings for the C++ library Elemental for distributed-memory dense and sparse-direct linear algebra and optimization which supports a wide range of functionality not available elsewhere. See more at http://libelemental.org/. The current wrappers support a subset of the functionality in Elemental but the process of adding functionality from Elemental is fairly simple.\n",
    "\n",
    "The `Elemental.jl` provides three different APIs for Elemental. The first API tries to mimic the API of the C++ library as much as possible which makes it easy to use Elemental's focumentation while working with Elemental.jl. The second API extends some of Julia's linear algebra functions to work on Elemental's array types. Finally, there `Elemental.jl` defines linear algebra methods methods for `DArrays` where a conversion to and from Elemental's array types happens implicitly.\n",
    "\n",
    "The library is based on MPI and hence an MPI installation is required before `Elemental.jl` can be installed. If the package `MPI.jl` is installed, it is possible to run MPI programs from within an interactive Julia session. In the following, we will show a few examples of using `Elemental.jl` interactively.\n",
    "\n",
    "At first, we load `MPI.jl`, define an `MPIManager` with four workers, launch the workers with the `addprocs` function, and finally load `Elemental.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MPI\n",
    "man = MPIManager(np = 4)\n",
    "addprocs(man)\n",
    "using Elemental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use the `@mpi_do` macro to execute expressions on the MPI workers. We first intialize a distributed Elemental matrix with `Float64` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mpi_do man A = Elemental.DistMatrix(Float64);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 2:\tElemental.DistMatrix{Float64}\n",
      "\tFrom worker 3:\tElemental.DistMatrix{Float64}\n",
      "\tFrom worker 5:\tElemental.DistMatrix{Float64}\n",
      "\tFrom worker 4:\tElemental.DistMatrix{Float64}\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man println(typeof(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we didn't specify the shape of the matrix. Following the C++ API, this can be done as part of the filling the matrix with values. We'll here fill it with Gaussian variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mpi_do man Elemental.gaussian!(A, 4000, 2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.381227 seconds (1.35 k allocations: 86.720 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time @mpi_do man vals = svdvals(A);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the largest value. Notice that it will be printed on all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 3:\t107.85799536311406\n",
      "\tFrom worker 5:\t107.85799536311406\n",
      "\tFrom worker 4:\t107.85799536311406\n",
      "\tFrom worker 2:\t107.85799536311406\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man println(vals[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative would be to work with `DArray` from the `DistributedArrays` package and have `Elemental.jl` do the conversion implicitly. That would instead look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000×2000 DistributedArrays.DArray{Float64,2,Array{Float64,2}}:\n",
       "  0.335499    0.318032    3.28741    0.592515   …  -1.31524     0.038773  \n",
       " -0.154335    0.280666    0.527864  -0.216495      -0.727533    0.126182  \n",
       "  1.36767    -0.0761083   0.648303  -0.68143       -2.58291     0.922192  \n",
       "  0.142799   -1.12469    -1.15906    0.849784       0.258645    0.329292  \n",
       " -1.31024    -1.28328     0.663565  -1.30362        1.51899     0.0446908 \n",
       "  0.102125    0.0112773  -2.28842    0.0440335  …   1.55138    -0.301289  \n",
       " -0.137411   -0.0379651  -0.673978   0.213859       1.61906    -0.460501  \n",
       " -0.624762    0.130119   -1.22233   -1.50813        0.867158    0.424131  \n",
       "  0.711469   -1.86969    -0.683314  -1.71519       -0.0936943  -0.917402  \n",
       " -0.0663845  -0.891755   -0.555031   0.0409641     -1.48966    -0.593472  \n",
       " -0.982027   -1.00371    -1.31893    1.29424    …  -0.527498    0.0630081 \n",
       "  0.275746   -0.152106   -0.807856   1.7084         0.57542     2.10345   \n",
       " -0.510743   -0.574049    0.145004   0.76524       -1.65914     1.21557   \n",
       "  ⋮                                             ⋱                         \n",
       "  0.218534    1.18987    -0.491367   1.40398       -0.445664   -1.1068    \n",
       "  1.10174    -0.702634   -0.539354  -0.333986      -0.304193    0.0629128 \n",
       " -0.059929    1.34985     1.3885     0.125057   …   0.628936    0.801676  \n",
       "  1.24654     0.0609617   0.794045   1.4373         0.314998   -1.00176   \n",
       " -1.72326    -2.34772    -1.82451    0.763724      -0.478347   -0.131442  \n",
       " -0.567922   -0.287506   -0.831479  -1.18022       -0.0191444  -2.07586   \n",
       " -1.35392     1.26832    -2.4372    -0.690453       0.507438   -0.568116  \n",
       "  0.289068    1.33433     1.00502    0.371423   …   1.51417     0.88789   \n",
       " -0.0155715  -2.41597    -0.599242  -0.381133      -0.232709   -1.76054   \n",
       " -0.218424   -0.750976   -1.82419   -0.825235      -2.26694    -1.65659   \n",
       "  1.58236     2.23356    -1.48618   -0.137598      -0.578368    0.313013  \n",
       " -0.662057   -0.620177    0.203023  -0.341927      -0.443957   -0.00237365"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD = DistributedArrays.drandn(4000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.278759 seconds (2.00 k allocations: 144.203 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000×1 DistributedArrays.DArray{Float64,2,Array{Float64,2}}:\n",
       " 107.989 \n",
       " 107.619 \n",
       " 107.069 \n",
       " 106.815 \n",
       " 106.677 \n",
       " 106.526 \n",
       " 106.155 \n",
       " 106.004 \n",
       " 105.668 \n",
       " 105.576 \n",
       " 105.449 \n",
       " 105.392 \n",
       " 105.314 \n",
       "   ⋮     \n",
       "  19.8653\n",
       "  19.7438\n",
       "  19.6669\n",
       "  19.6284\n",
       "  19.58  \n",
       "  19.4592\n",
       "  19.4138\n",
       "  19.2964\n",
       "  19.0615\n",
       "  18.9466\n",
       "  18.817 \n",
       "  18.6887"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time svdvals(AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the advantage that `DArray` are more similar to normal Julia arrays and therefore convenient to work with. However, the data has to be copied back and forth between the two different ditributed array representations so there is some overhead associated with the convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
